---
layout: default
title: Rajiv Khanna -- Home
---
<div >
    <p>I am on the job market this year! Please reach out if you feel an opportunity matches my profile.[<a href="./researchstatement.pdf"> Research Statement </a> ] [<a href="./teaching.pdf"> Teaching Statement</a>] [<a href="./diversity.pdf"> Statement on Diversity and Inclusion</a>] 
    
	<p> I am a Postdoctoral Scholar at the <a href="https://foda.berkeley.edu"> Foundations of Data Analystics Institute</a> at University of California, Berkeley and am very fortunate to be working with <a href=https://www.stat.berkeley.edu/~mmahoney/>Michael Mahoney</a>. 

    <p> Previously, I was a Research Fellow in the <a href="https://simons.berkeley.edu/programs/dl2019">Foundations of Data Science</a> program at the <a href="https://simons.berkeley.edu/">Simons Institute</a> in Fall 2018. I graduated with my PhD from Intelligent Data Exploration and Analysis Laboratory (<a href="http://www.ideal.ece.utexas.edu">IDEAL</a>) at <a href="http://www.utexas.edu/">UT Austin</a>. I am very fortunate to be able to learn from and interact with Professors <a href="http://www.ideal.ece.utexas.edu/ghosh">Joydeep Ghosh</a>, and <a href="http://users.ece.utexas.edu/~dimakis/"> Alex Dimakis</a>. My PhD research is to study various aspects of greedy algorithms in machine learning, both from theory and practical viewpoints. From the theory side, I have worked on studying approximation guarantees for "greedy-like" algorithms for sparsity and rank constrained problems for general functions based on their smoothness and convexity, and convergence rates for greedy algorithms like accelerated IHT, Matching Pursuit and boosting . From practical side, I have worked on using greedy selections for approximate variational inference for sparse regression, sparse PCA for fMRI applications, and for interpreting black box models. In the past I have worked on problems involving predicting buying propensity based on marketing touches, large scale recommendation systems, Ad Click prediction, and ranking.

</div>
